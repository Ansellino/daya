{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Copy of Model Maker Question Answer Tutorial",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddhPiM48hfY1"
      },
      "source": [
        "# Import Lib"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XtxiUeZEiXpt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f91d5b8b-9b65-47f9-bcd4-ecc6804aed85"
      },
      "source": [
        "!pip install -q tflite-model-maker\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "import tensorflow as tf\n",
        "assert tf.__version__.startswith('2')\n",
        "\n",
        "from tflite_model_maker import model_spec\n",
        "from tflite_model_maker import question_answer\n",
        "from tflite_model_maker.config import ExportFormat\n",
        "from tflite_model_maker.question_answer import DataLoader\n",
        "from tflite_model_maker.config import QuantizationConfig"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 593kB 2.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.2MB 16.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 112kB 20.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 122kB 15.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 71kB 8.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 686kB 19.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 645kB 19.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 6.3MB 19.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 174kB 52.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 849kB 53.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 92kB 12.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1MB 50.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 38.2MB 76kB/s \n",
            "\u001b[K     |████████████████████████████████| 102kB 13.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 358kB 42.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 194kB 55.4MB/s \n",
            "\u001b[?25h  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for py-cpuinfo (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZXOUbnZhmy-"
      },
      "source": [
        "# Load Model and Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQzSB74EhlFJ"
      },
      "source": [
        "spec = model_spec.get('mobilebert_qa')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7tOfUr2KlgpU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5277d39-1231-47a8-f88a-6378e7e6df78"
      },
      "source": [
        "# Follow TF Lite Bert Tutorial as reference and dataset\n",
        "train_data_path = tf.keras.utils.get_file(\n",
        "    fname='triviaqa-web-train-8000.json',\n",
        "    origin='https://storage.googleapis.com/download.tensorflow.org/models/tflite/dataset/triviaqa-web-train-8000.json')\n",
        "validation_data_path = tf.keras.utils.get_file(\n",
        "    fname='triviaqa-verified-web-dev.json',\n",
        "    origin='https://storage.googleapis.com/download.tensorflow.org/models/tflite/dataset/triviaqa-verified-web-dev.json')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/models/tflite/dataset/triviaqa-web-train-8000.json\n",
            "32571392/32570663 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/models/tflite/dataset/triviaqa-verified-web-dev.json\n",
            "1171456/1167744 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_fOlZsklmlL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c52ce441-9a99-4129-b0b9-6e15e33e1b1b"
      },
      "source": [
        "train_data = DataLoader.from_squad(train_data_path, spec, is_training=True)\n",
        "validation_data = DataLoader.from_squad(validation_data_path, spec, is_training=False)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:AutoGraph could not transform <bound method BertQAModelSpec.select_data_from_record of <tensorflow_examples.lite.model_maker.core.task.model_spec.text_spec.BertQAModelSpec object at 0x7f1990996790>> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: '<' not supported between instances of 'str' and 'Literal'\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:AutoGraph could not transform <bound method BertQAModelSpec.select_data_from_record of <tensorflow_examples.lite.model_maker.core.task.model_spec.text_spec.BertQAModelSpec object at 0x7f1990996790>> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: '<' not supported between instances of 'str' and 'Literal'\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING: AutoGraph could not transform <bound method BertQAModelSpec.select_data_from_record of <tensorflow_examples.lite.model_maker.core.task.model_spec.text_spec.BertQAModelSpec object at 0x7f1990996790>> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: '<' not supported between instances of 'str' and 'Literal'\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nuFt_ZdgiNDI"
      },
      "source": [
        "# Transfer Learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TvYSUuJY3QxR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c994f22-b3f3-47ae-e74c-0e33917f26fe"
      },
      "source": [
        "model = question_answer.create(train_data, model_spec=spec, epochs=10)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Retraining the models...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Retraining the models...\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1067/1067 [==============================] - 720s 632ms/step - loss: 2.1951 - start_positions_loss: 2.2233 - end_positions_loss: 2.1669\n",
            "Epoch 2/10\n",
            "1067/1067 [==============================] - 675s 632ms/step - loss: 0.9977 - start_positions_loss: 0.9951 - end_positions_loss: 1.0003\n",
            "Epoch 3/10\n",
            "1067/1067 [==============================] - 675s 632ms/step - loss: 0.6215 - start_positions_loss: 0.6201 - end_positions_loss: 0.6230\n",
            "Epoch 4/10\n",
            "1067/1067 [==============================] - 675s 632ms/step - loss: 0.3371 - start_positions_loss: 0.3370 - end_positions_loss: 0.3372\n",
            "Epoch 5/10\n",
            "1067/1067 [==============================] - 675s 633ms/step - loss: 0.1795 - start_positions_loss: 0.1802 - end_positions_loss: 0.1788\n",
            "Epoch 6/10\n",
            "1067/1067 [==============================] - 676s 633ms/step - loss: 0.0944 - start_positions_loss: 0.0948 - end_positions_loss: 0.0940\n",
            "Epoch 7/10\n",
            "1067/1067 [==============================] - 675s 633ms/step - loss: 0.0476 - start_positions_loss: 0.0481 - end_positions_loss: 0.0470\n",
            "Epoch 8/10\n",
            "1067/1067 [==============================] - 675s 633ms/step - loss: 0.0234 - start_positions_loss: 0.0234 - end_positions_loss: 0.0234\n",
            "Epoch 9/10\n",
            "1067/1067 [==============================] - 675s 633ms/step - loss: 0.0101 - start_positions_loss: 0.0099 - end_positions_loss: 0.0103\n",
            "Epoch 10/10\n",
            "1067/1067 [==============================] - 676s 633ms/step - loss: 0.0030 - start_positions_loss: 0.0037 - end_positions_loss: 0.0022\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gd7Hs8TF8n3H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a01df46b-2778-4fb3-967d-e9e765f12e9b"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"bert_span_labeler\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_word_ids (InputLayer)     [(None, 384)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_mask (InputLayer)         [(None, 384)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_type_ids (InputLayer)     [(None, 384)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "core_model (Functional)         [(None, 384, 512), ( 24581888    input_word_ids[0][0]             \n",
            "                                                                 input_mask[0][0]                 \n",
            "                                                                 input_type_ids[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "span_labeling (SpanLabeling)    [(None, None), (None 1026        core_model[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "start_positions (Lambda)        (None, 384)          0           span_labeling[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "end_positions (Lambda)          (None, 384)          0           span_labeling[0][1]              \n",
            "==================================================================================================\n",
            "Total params: 24,582,914\n",
            "Trainable params: 24,582,914\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Nsyr0YYiW3d"
      },
      "source": [
        "# Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A8c2ZQ0J3Riy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9df29228-66a8-469d-fb10-aa5879cd239c"
      },
      "source": [
        "model.evaluate(validation_data)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Made predictions for 200 records.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Made predictions for 200 records.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Made predictions for 400 records.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Made predictions for 400 records.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Made predictions for 600 records.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Made predictions for 600 records.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Made predictions for 800 records.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Made predictions for 800 records.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Made predictions for 1000 records.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Made predictions for 1000 records.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Made predictions for 1200 records.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Made predictions for 1200 records.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'exact_match': 0.5136054421768708, 'final_f1': 0.6083441313033151}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trPboPJMif1j"
      },
      "source": [
        "# Export Model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZ3VKQ2JsNAU"
      },
      "source": [
        "config = QuantizationConfig.for_float16()"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ip2KvL5ssOmZ",
        "outputId": "c2642549-def0-485d-9d26-b389d3c9e289"
      },
      "source": [
        "model.export(export_dir='.', export_format=ExportFormat.TFLITE, quantization_config=config)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmpy3gu4tmy/saved_model/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmpy3gu4tmy/saved_model/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Vocab file is inside the TFLite model with metadata.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Vocab file is inside the TFLite model with metadata.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saved vocabulary in /tmp/tmpoewucfkl/vocab.txt.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saved vocabulary in /tmp/tmpoewucfkl/vocab.txt.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished populating metadata and associated file to the model:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished populating metadata and associated file to the model:\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:./model.tflite\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:./model.tflite\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:The associated file that has been been packed to the model is:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:The associated file that has been been packed to the model is:\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:['vocab.txt']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:['vocab.txt']\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:TensorFlow Lite model exported successfully: ./model.tflite\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:TensorFlow Lite model exported successfully: ./model.tflite\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ochbq95ZrVFX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b459a7a-dc8d-4fbf-a3ea-b92450f08b9a"
      },
      "source": [
        "model.evaluate_tflite('model.tflite', validation_data)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Made predictions for 100 records.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Made predictions for 100 records.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Made predictions for 200 records.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Made predictions for 200 records.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Made predictions for 300 records.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Made predictions for 300 records.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Made predictions for 400 records.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Made predictions for 400 records.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Made predictions for 500 records.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Made predictions for 500 records.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Made predictions for 600 records.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Made predictions for 600 records.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Made predictions for 700 records.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Made predictions for 700 records.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Made predictions for 800 records.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Made predictions for 800 records.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Made predictions for 900 records.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Made predictions for 900 records.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Made predictions for 1000 records.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Made predictions for 1000 records.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Made predictions for 1100 records.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Made predictions for 1100 records.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Made predictions for 1200 records.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Made predictions for 1200 records.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'exact_match': 0.5136054421768708, 'final_f1': 0.6083441313033151}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    }
  ]
}