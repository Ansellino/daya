{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Model Maker Question Answer V2",
      "provenance": [],
      "collapsed_sections": [
        "ddhPiM48hfY1"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3N6eEZhXfrqd"
      },
      "source": [
        "## Using Below URL as Reference\n",
        "https://www.tensorflow.org/lite/tutorials/model_maker_question_answer \n",
        "\n",
        "https://arxiv.org/abs/1810.04805"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddhPiM48hfY1"
      },
      "source": [
        "# Import Lib"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XtxiUeZEiXpt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d84dfe03-a331-49dc-bab8-649ec3e5bb33"
      },
      "source": [
        "!pip install -q tflite-model-maker\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow import keras\n",
        "from tflite_model_maker import model_spec\n",
        "from tflite_model_maker import question_answer\n",
        "from tflite_model_maker.config import ExportFormat\n",
        "from tflite_model_maker.question_answer import DataLoader\n",
        "from tflite_model_maker.config import QuantizationConfig"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 593kB 8.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 686kB 19.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 122kB 24.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 174kB 25.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 645kB 45.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 849kB 50.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 6.3MB 52.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 112kB 55.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1MB 49.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 71kB 10.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 92kB 13.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.2MB 45.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 102kB 13.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 38.2MB 76kB/s \n",
            "\u001b[K     |████████████████████████████████| 358kB 43.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 194kB 52.7MB/s \n",
            "\u001b[?25h  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for py-cpuinfo (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZXOUbnZhmy-"
      },
      "source": [
        "# Load Model and Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z0oo-zxmehp3"
      },
      "source": [
        "# https://www.tensorflow.org/lite/api_docs/python/tflite_model_maker/question_answer/BertQaSpec\n",
        "# Follow https://arxiv.org/abs/1810.04805 BERT authors recommendations for fine-tuning\n",
        "# Change 4e-05 into 2e-5\n",
        "# predict_batch_size 8 into 16\n",
        "spec = question_answer.MobileBertQaSpec(\n",
        "    uri='https://tfhub.dev/google/mobilebert/uncased_L-24_H-128_B-512_A-4_F-4_OPT/1',\n",
        "    model_dir=None, seq_len=512, query_len=64, doc_stride=128,\n",
        "    dropout_rate=0.1, initializer_range=0.02, learning_rate=3e-5,\n",
        "    distribution_strategy='off', num_gpus=-1, tpu='',\n",
        "    trainable=True, predict_batch_size=16, do_lower_case=True, is_tf2=False,\n",
        "    tflite_input_name=None, tflite_output_name=None, init_from_squad_model=False,\n",
        "    default_batch_size=32, name='MobileBert'\n",
        ")"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7tOfUr2KlgpU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e30ed924-3343-4b84-d2c0-345cbf7a87b4"
      },
      "source": [
        "# Follow TF Lite Bert Tutorial as reference and dataset\n",
        "train_data_path = tf.keras.utils.get_file(\n",
        "    fname='triviaqa-web-train-8000.json',\n",
        "    origin='https://storage.googleapis.com/download.tensorflow.org/models/tflite/dataset/triviaqa-web-train-8000.json')\n",
        "validation_data_path = tf.keras.utils.get_file(\n",
        "    fname='triviaqa-verified-web-dev.json',\n",
        "    origin='https://storage.googleapis.com/download.tensorflow.org/models/tflite/dataset/triviaqa-verified-web-dev.json')\n",
        "\n",
        "#Revert to default due need long time to train dataset with nested\n",
        "#train_data_path = tf.keras.utils.get_file(\n",
        "#    fname='QuAC-train.json',\n",
        "#    origin='https://s3.amazonaws.com/my89public/quac/train_v0.2.json')\n",
        "#validation_data_path = tf.keras.utils.get_file(\n",
        "#    fname='QuAC-val.json',\n",
        "#    origin='https://s3.amazonaws.com/my89public/quac/val_v0.2.json')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/models/tflite/dataset/triviaqa-web-train-8000.json\n",
            "32571392/32570663 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/models/tflite/dataset/triviaqa-verified-web-dev.json\n",
            "1171456/1167744 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I_fOlZsklmlL",
        "outputId": "6d3451de-6e5e-48de-bec2-03c6fd1150a9"
      },
      "source": [
        "train_data = DataLoader.from_squad(train_data_path, spec, is_training=True)\n",
        "validation_data = DataLoader.from_squad(validation_data_path, spec, is_training=False)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:AutoGraph could not transform <bound method BertQAModelSpec.select_data_from_record of <tensorflow_examples.lite.model_maker.core.task.model_spec.text_spec.BertQAModelSpec object at 0x7ff16ddfca50>> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: '<' not supported between instances of 'str' and 'Literal'\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:AutoGraph could not transform <bound method BertQAModelSpec.select_data_from_record of <tensorflow_examples.lite.model_maker.core.task.model_spec.text_spec.BertQAModelSpec object at 0x7ff16ddfca50>> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: '<' not supported between instances of 'str' and 'Literal'\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING: AutoGraph could not transform <bound method BertQAModelSpec.select_data_from_record of <tensorflow_examples.lite.model_maker.core.task.model_spec.text_spec.BertQAModelSpec object at 0x7ff16ddfca50>> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: '<' not supported between instances of 'str' and 'Literal'\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nuFt_ZdgiNDI"
      },
      "source": [
        "# Transfer Learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e-O_iFMXefqx",
        "outputId": "60571e14-1638-4cf1-da4d-5d236d222225"
      },
      "source": [
        "#Use shuffle for random distribution\n",
        "model = question_answer.create(train_data, model_spec=spec, epochs=4,shuffle=True)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Retraining the models...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Retraining the models...\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "859/859 [==============================] - 1203s 1s/step - loss: 2.5519 - start_positions_loss: 2.7787 - end_positions_loss: 2.3251\n",
            "Epoch 2/4\n",
            "859/859 [==============================] - 1167s 1s/step - loss: 1.1678 - start_positions_loss: 1.1633 - end_positions_loss: 1.1722\n",
            "Epoch 3/4\n",
            "859/859 [==============================] - 1168s 1s/step - loss: 0.7722 - start_positions_loss: 0.7728 - end_positions_loss: 0.7716\n",
            "Epoch 4/4\n",
            "859/859 [==============================] - 1167s 1s/step - loss: 0.5366 - start_positions_loss: 0.5382 - end_positions_loss: 0.5350\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gd7Hs8TF8n3H",
        "outputId": "6a05cfda-de77-48b8-9527-5df423fedc36"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"bert_span_labeler\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_word_ids (InputLayer)     [(None, 512)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_mask (InputLayer)         [(None, 512)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_type_ids (InputLayer)     [(None, 512)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "core_model (Functional)         [(None, 512, 512), ( 24581888    input_word_ids[0][0]             \n",
            "                                                                 input_mask[0][0]                 \n",
            "                                                                 input_type_ids[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "span_labeling (SpanLabeling)    [(None, None), (None 1026        core_model[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "start_positions (Lambda)        (None, 512)          0           span_labeling[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "end_positions (Lambda)          (None, 512)          0           span_labeling[0][1]              \n",
            "==================================================================================================\n",
            "Total params: 24,582,914\n",
            "Trainable params: 24,582,914\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Nsyr0YYiW3d"
      },
      "source": [
        "# Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A8c2ZQ0J3Riy",
        "outputId": "cc49157a-5306-4f41-a75d-5ec87b5077bd"
      },
      "source": [
        "model.evaluate(validation_data)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Made predictions for 400 records.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Made predictions for 400 records.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Made predictions for 800 records.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Made predictions for 800 records.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'exact_match': 0.5476190476190477, 'final_f1': 0.6288730571593717}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trPboPJMif1j"
      },
      "source": [
        "# Export Model "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wsrXKIZfrnrW"
      },
      "source": [
        "Reduce model size and inference latency by using QuantizationConfig"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ip2KvL5ssOmZ",
        "outputId": "3a64e903-596c-4d08-bb1e-996eabad8216"
      },
      "source": [
        "config = QuantizationConfig.for_float16()\n",
        "print('Saving model...')\n",
        "model.export(export_dir='.', export_format=ExportFormat.TFLITE, quantization_config=config)\n",
        "model.evaluate_tflite('model.tflite', validation_data)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saving model...\n",
            "INFO:tensorflow:Assets written to: /tmp/tmpp1s0eibp/saved_model/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmpp1s0eibp/saved_model/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Vocab file is inside the TFLite model with metadata.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Vocab file is inside the TFLite model with metadata.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saved vocabulary in /tmp/tmpftay63c2/vocab.txt.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saved vocabulary in /tmp/tmpftay63c2/vocab.txt.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished populating metadata and associated file to the model:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished populating metadata and associated file to the model:\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:./model.tflite\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:./model.tflite\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:The associated file that has been been packed to the model is:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:The associated file that has been been packed to the model is:\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:['vocab.txt']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:['vocab.txt']\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:TensorFlow Lite model exported successfully: ./model.tflite\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:TensorFlow Lite model exported successfully: ./model.tflite\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Made predictions for 100 records.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Made predictions for 100 records.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Made predictions for 200 records.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Made predictions for 200 records.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Made predictions for 300 records.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Made predictions for 300 records.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Made predictions for 400 records.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Made predictions for 400 records.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Made predictions for 500 records.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Made predictions for 500 records.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Made predictions for 600 records.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Made predictions for 600 records.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Made predictions for 700 records.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Made predictions for 700 records.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Made predictions for 800 records.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Made predictions for 800 records.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Made predictions for 900 records.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Made predictions for 900 records.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'exact_match': 0.5476190476190477, 'final_f1': 0.6288730571593717}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    }
  ]
}